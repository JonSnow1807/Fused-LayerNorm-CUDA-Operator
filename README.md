# Fused LayerNorm CUDA Operator for PyTorch

A counter-intuitive discovery: Making LayerNorm **1.86-2.66x faster** by REMOVING optimizations.

## ðŸŽ¯ Key Achievement

We achieve **1.86x-2.66x speedup** over PyTorch's native LayerNorm by simplifying the implementation, not complexifying it. Our journey revealed that LayerNorm is latency-bound, not bandwidth-bound, using only ~10% of GPU memory bandwidth.

## ðŸ“Š Performance Results

Benchmarked on NVIDIA A100, PyTorch 2.7.1, CUDA 12.8:

### Two Scenarios Measured

| Scenario | Average Speedup | Description |
|----------|----------------|-------------|
| Realistic | **1.86x** | Different input tensors (typical inference) |
| Optimal | **2.66x** | Cached tensor reuse (best-case) |

### Detailed Performance by Model Size

| Configuration | Realistic Speedup | Optimal Speedup |
|--------------|-------------------|-----------------|
| BERT (32Ã—768) | 2.36x | 2.57x |
| GPT-2 (32Ã—1024) | 1.73x | 2.61x |
| GPT-3 (32Ã—4096) | 1.64x | 2.64x |
| Large Batch (64Ã—4096) | 1.74x | 2.67x |
| Odd Dimensions (17Ã—1023) | 1.81x | 2.83x |

Run `python benchmarks/reproduce_speedup.py` to verify these results on your hardware.

## ðŸ” The Discovery: Simple > Complex

### What We Found

- LayerNorm at typical sizes (768-4096) uses only **10.7%** of memory bandwidth
- We're **latency-bound**, not bandwidth-bound
- Complex optimizations add latency without improving throughput
- Removing vectorization made it **faster**!

### Our Approach

```cuda
// Instead of complex float4 vectorization...
// We use simple, clean loops:
for (int i = tid; i < N; i += blockDim.x) {
    sum += X[i];
}
```

**Result:** Less code â†’ Fewer instructions â†’ Lower latency â†’ **Faster execution**

## ðŸš€ Key Features

- âœ… **1.86-2.66x faster** than PyTorch's optimized implementation
- âœ… **~100 lines** of simple CUDA (vs complex optimizations)
- âœ… **Universal compatibility** - works on ANY dimension (odd, prime, power-of-2)
- âœ… **Better numerical accuracy** - 4.77e-07 maximum error
- âœ… **Fused variants** - LayerNorm + GELU in single kernel
- âœ… **Production ready** - extensively tested across all edge cases

## ðŸ”§ Installation

### Prerequisites

- CUDA Toolkit >= 11.0
- PyTorch >= 1.9.0
- Python >= 3.7

### Install from Source

```bash
# Clone the repository
git clone https://github.com/JonSnow1807/Fused-LayerNorm-CUDA-Operator.git
cd Fused-LayerNorm-CUDA-Operator

# Install the package
pip install -e .
```

## ðŸ“– Usage

### Basic Usage

```python
import torch
import fused_layernorm_cuda

# Prepare inputs
x = torch.randn(32, 4096, device='cuda')
gamma = torch.ones(4096, device='cuda')
beta = torch.zeros(4096, device='cuda')

# Run our optimized LayerNorm
output = fused_layernorm_cuda.layernorm(x, gamma, beta, eps=1e-5)

# Fused LayerNorm + GELU
output_gelu = fused_layernorm_cuda.layernorm_gelu(x, gamma, beta, eps=1e-5)
```

### Drop-in Replacement for nn.LayerNorm

```python
import torch.nn as nn

# Replace PyTorch LayerNorm in existing models
def replace_layernorm(module):
    for name, child in module.named_children():
        if isinstance(child, nn.LayerNorm):
            # Create our optimized version
            optimized = lambda x: fused_layernorm_cuda.layernorm(
                x, child.weight, child.bias, child.eps
            )
            setattr(module, name, optimized)
        else:
            replace_layernorm(child)

# Apply to any model
model = nn.TransformerEncoderLayer(d_model=4096, nhead=16)
replace_layernorm(model)
# Now 1.86-2.66x faster!
```

## ðŸ§ª Validation & Testing

### Verify Correctness

```bash
python tests/test_correctness.py       # Basic correctness
python tests/edge_cases.py             # Edge cases (1Ã—1 to 200Ã—10000)
python tests/numerical_validation.py   # Numerical accuracy
```

### Reproduce Performance

```bash
# See both realistic and optimal scenarios
python benchmarks/reproduce_speedup.py

# Statistical validation (p < 0.0001)
python tests/statistical_validation.py
```

## ðŸ“ˆ Why This Works

### The Latency vs Bandwidth Insight

Traditional GPU optimization assumes memory bandwidth is the bottleneck. Our profiling revealed:

- **Memory Bandwidth Utilization:** 10.7%
- **Actual Bandwidth:** 167 GB/s
- **A100 Peak:** 1555 GB/s
- **We're using only 1/10th of available bandwidth!**

This means:
- Complex memory access patterns don't help
- Vectorization adds overhead without benefit
- Simple sequential access is optimal
- Instruction count matters more than memory pattern

### Kernel Simplicity Comparison

| Metric | PyTorch | Ours | Improvement |
|--------|---------|------|-------------|
| Lines of Code | ~500 | ~100 | 80% less |
| Instruction Complexity | High | Low | Simpler |
| Register Pressure | High | Low | Better occupancy |
| Edge Case Handling | Complex | Simple | Universal |

## ðŸ›ï¸ Project Structure

```
Fused-LayerNorm-CUDA-Operator/
â”œâ”€â”€ csrc/
â”‚   â”œâ”€â”€ layernorm_cuda_kernel.cu  # Simple, fast kernel
â”‚   â””â”€â”€ bindings.cpp               # Python bindings
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ reproduce_speedup.py       # Transparent benchmark
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ statistical_validation.py  # Statistical proof (p<0.0001)
â”‚   â”œâ”€â”€ edge_cases.py              # All dimension tests
â”‚   â””â”€â”€ numerical_validation.py    # Accuracy verification
â”œâ”€â”€ setup.py                       # Build configuration
â””â”€â”€ README.md                      # This file
```

## ðŸ”¬ Technical Details

### The Winning Kernel Design

```cuda
template<typename scalar_t>
__global__ void layernorm_kernel(...) {
    // Simple reduction for mean
    float sum = 0.0f;
    for (int i = tid; i < N; i += blockDim.x) {
        sum += X[i];
    }
    sum = blockReduceSum(sum);
    
    // Simple reduction for variance
    float variance_sum = 0.0f;
    for (int i = tid; i < N; i += blockDim.x) {
        float diff = X[i] - mean;
        variance_sum += diff * diff;
    }
    variance_sum = blockReduceSum(variance_sum);
    
    // Simple normalization
    for (int i = tid; i < N; i += blockDim.x) {
        Y[i] = (X[i] - mean) * rsqrt(variance + eps) * gamma[i] + beta[i];
    }
}
```

**That's it.** No complex optimizations. Just clean, simple code.

### Statistical Validation

All results validated with:
- 30 samples per configuration
- Statistical significance: p < 0.0001
- Tested across 12+ configurations
- Edge cases from 1Ã—1 to 200Ã—10000

## ðŸ“š Citation

If you find this work useful or learn from our approach:

```bibtex
@software{fused_layernorm_cuda_2025,
  title = {Simple Beats Complex: Achieving 1.86-2.66x LayerNorm Speedup by Removing Optimizations},
  author = {Chinmay Shrivastava},
  year = {2025},
  url = {https://github.com/JonSnow1807/Fused-LayerNorm-CUDA-Operator},
  note = {Counter-intuitive optimization: simpler code runs faster}
}
```

## ðŸŽ“ Lessons Learned

1. **Profile First**: Don't assume - measure! We found 10.7% bandwidth usage
2. **Question Dogma**: "GPUs are bandwidth-bound" - not always!
3. **Simplicity Wins**: Less code can mean better performance
4. **Latency Matters**: At low bandwidth usage, optimize for instruction count

## ðŸ¤ Contributing

We welcome contributions! Especially interested in:
- Testing on other GPUs (V100, H100, RTX series)
- Backward pass implementation
- Integration with popular frameworks

## ðŸ“ License

MIT License - see LICENSE file for details.

## ðŸ™ Acknowledgments

- The bug that led to enlightenment (float4 breaking on odd dimensions)
- The PyTorch team for the baseline to beat
- The CUDA community for optimization wisdom (that we learned to ignore!)

---

**Remember:** Sometimes the best optimization is no optimization. Profile, measure, and let data guide you.

> "Perfection is achieved not when there is nothing more to add, but when there is nothing left to take away." - Antoine de Saint-ExupÃ©ry
