Fused LayerNorm CUDA Operator for PyTorch






A counter-intuitive discovery: Making LayerNorm 1.86â€“2.66Ã— faster by removing optimizations.

ðŸŽ¯ Key Achievement

We achieve 1.86Ã—â€“2.66Ã— speedup over PyTorchâ€™s native LayerNorm by simplifying the implementation, not complexifying it. Our profiling shows LayerNorm is latency-bound, not bandwidth-bound, using only ~10% of GPU memory bandwidth.

ðŸ“Š Performance Results

Benchmarked on: NVIDIA A100 â€¢ PyTorch 2.7.1 â€¢ CUDA 12.8

Two Scenarios Measured
Scenario	Average Speedup	Description
Realistic	1.86Ã—	Different input tensors (typical inference)
Optimal	2.66Ã—	Cached tensor reuse (best-case)
Detailed Performance by Model Size
Configuration	Realistic Speedup	Optimal Speedup
BERT (32Ã—768)	2.36Ã—	2.57Ã—
GPT-2 (32Ã—1024)	1.73Ã—	2.61Ã—
GPT-3 (32Ã—4096)	1.64Ã—	2.64Ã—
Large Batch (64Ã—4096)	1.74Ã—	2.67Ã—
Odd Dimensions (17Ã—1023)	1.81Ã—	2.83Ã—

Verify on your machine:

python benchmarks/reproduce_speedup.py

ðŸ” The Discovery: Simple > Complex

What We Found

Typical sizes (768â€“4096) use only 10.7% of memory bandwidth

Weâ€™re latency-bound, not bandwidth-bound

Complex optimizations add latency without improving throughput

Removing vectorization made it faster

Our Approach

// Instead of complex float4 vectorization...
// We use simple, clean loops:
for (int i = tid; i < N; i += blockDim.x) {
    sum += X[i];
}


Result: Less code â†’ Fewer instructions â†’ Lower latency â†’ Faster execution

ðŸš€ Key Features

âœ… 1.86â€“2.66Ã— faster than PyTorchâ€™s optimized implementation

âœ… ~100 lines of simple CUDA (vs complex optimizations)

âœ… Universal compatibility â€” any dimension (odd, prime, power-of-2)

âœ… Better numerical accuracy â€” 4.77e-07 maximum error

âœ… Fused variants â€” LayerNorm + GELU in a single kernel

âœ… Production ready â€” extensively tested across edge cases

ðŸ”§ Installation

Prerequisites

CUDA Toolkit â‰¥ 11.0

PyTorch â‰¥ 1.9.0

Python â‰¥ 3.7

Install from Source

# Clone the repository
git clone https://github.com/JonSnow1807/Fused-LayerNorm-CUDA-Operator.git
cd Fused-LayerNorm-CUDA-Operator

# Install the package
pip install -e .

ðŸ“– Usage

Basic Usage

import torch
import fused_layernorm_cuda

# Prepare inputs
x = torch.randn(32, 4096, device='cuda')
gamma = torch.ones(4096, device='cuda')
beta = torch.zeros(4096, device='cuda')

# Run our optimized LayerNorm
output = fused_layernorm_cuda.layernorm(x, gamma, beta, eps=1e-5)

# Fused LayerNorm + GELU
output_gelu = fused_layernorm_cuda.layernorm_gelu(x, gamma, beta, eps=1e-5)


Drop-in Replacement for nn.LayerNorm

import torch.nn as nn

def replace_layernorm(module):
    for name, child in module.named_children():
        if isinstance(child, nn.LayerNorm):
            optimized = lambda x: fused_layernorm_cuda.layernorm(
                x, child.weight, child.bias, child.eps
            )
            setattr(module, name, optimized)
        else:
            replace_layernorm(child)

# Apply to any model
model = nn.TransformerEncoderLayer(d_model=4096, nhead=16)
replace_layernorm(model)
# Now 1.86â€“2.66Ã— faster!

ðŸ§ª Validation & Testing

Verify Correctness

python tests/test_correctness.py       # Basic correctness
python tests/edge_cases.py             # Edge cases (1Ã—1 to 200Ã—10000)
python tests/numerical_validation.py   # Numerical accuracy


Reproduce Performance

# See both realistic and optimal scenarios
python benchmarks/reproduce_speedup.py

# Statistical validation (p < 0.0001)
python tests/statistical_validation.py

ðŸ“ˆ Why This Works

Latency vs Bandwidth Insight

Memory Bandwidth Utilization: 10.7%

Actual Bandwidth: 167 GB/s

A100 Peak: 1555 GB/s

Weâ€™re using only ~1/10th of available bandwidth, so:

Complex memory access patterns donâ€™t help

Vectorization adds overhead without benefit

Simple sequential access is optimal

Instruction count dominates performance

Kernel Simplicity Comparison

Metric	PyTorch	Ours	Improvement
Lines of Code	~500	~100	80% less
Instruction Complexity	High	Low	Simpler
Register Pressure	High	Low	Better occupancy
Edge Case Handling	Complex	Simple	Universal
ðŸ›ï¸ Project Structure
Fused-LayerNorm-CUDA-Operator/
â”œâ”€â”€ csrc/
â”‚   â”œâ”€â”€ layernorm_cuda_kernel.cu  # Simple, fast kernel
â”‚   â””â”€â”€ bindings.cpp              # Python bindings
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ reproduce_speedup.py      # Transparent benchmark
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ statistical_validation.py # Statistical proof (p<0.0001)
â”‚   â”œâ”€â”€ edge_cases.py             # All dimension tests
â”‚   â””â”€â”€ numerical_validation.py   # Accuracy verification
â”œâ”€â”€ setup.py                      # Build configuration
â””â”€â”€ README.md                     # This file

ðŸ”¬ Technical Details

Winning Kernel Design

template<typename scalar_t>
__global__ void layernorm_kernel(...) {
    // Simple reduction for mean
    float sum = 0.0f;
    for (int i = tid; i < N; i += blockDim.x) {
        sum += X[i];
    }
    sum = blockReduceSum(sum);

    // Simple reduction for variance
    float variance_sum = 0.0f;
    for (int i = tid; i < N; i += blockDim.x) {
        float diff = X[i] - mean;
        variance_sum += diff * diff;
    }
    variance_sum = blockReduceSum(variance_sum);

    // Simple normalization
    for (int i = tid; i < N; i += blockDim.x) {
        Y[i] = (X[i] - mean) * rsqrt(variance + eps) * gamma[i] + beta[i];
    }
}


Statistical Validation

30 samples per configuration

Statistical significance: p < 0.0001

Tested across 12+ configurations

Edge cases from 1Ã—1 to 200Ã—10000

ðŸ“š Citation

If you find this work useful or learn from our approach:

@software{fused_layernorm_cuda_2025,
  title = {Simple Beats Complex: Achieving 1.86â€“2.66x LayerNorm Speedup by Removing Optimizations},
  author = {Chinmay Shrivastava},
  year = {2025},
  url = {https://github.com/JonSnow1807/Fused-LayerNorm-CUDA-Operator},
  note = {Counter-intuitive optimization: simpler code runs faster}
}

ðŸŽ“ Lessons Learned

Profile First â€” donâ€™t assume; measure (we found 10.7% bandwidth usage)

Question Dogma â€” â€œGPUs are bandwidth-boundâ€ isnâ€™t universal

Simplicity Wins â€” less code can mean better performance

Latency Matters â€” optimize instruction count at low bandwidth usage

ðŸ¤ Contributing

Contributions welcome! Especially:

Testing on other GPUs (V100, H100, RTX series)

Backward pass implementation

Integration with popular frameworks

ðŸ“ License

MIT License â€” see LICENSE for details.

ðŸ™ Acknowledgments

The bug that led to enlightenment (float4 breaking on odd dimensions)

The PyTorch team for the baseline to beat

The CUDA community for optimization wisdom (that we learned to ignore!)

Remember: Sometimes the best optimization is no optimization. Profile, measure, and let data guide you.
â€œPerfection is achieved not when there is nothing more to add, but when there is nothing left to take away.â€ â€” Antoine de Saint-ExupÃ©ry
